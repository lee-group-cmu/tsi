---
layout: default
---

# Supplement on 2D synthetic examples

The synthetic example in Figure~\ref{fig:2D_example_well_specified} showcases the main properties of our framework --- i.e., reliability (in the form of correct coverage) and precision (in the form of optimal constraining power) --- for an inference task that was introduced in \citesupp{sisson_sequential_2007} and has become a standard benchmark in the SBI literature \citepsupp{clarte_componentwise_2021,toni_approximate_2009,simola_adaptive_2021,lueckmann_benchmarking_2021}. It consists of estimating the (common) mean of the components of a two-dimensional Gaussian mixture, with one component having much broader covariance: $$X \mid \theta \sim \frac{1}{2}\mathcal{N}(\theta, I) + \frac{1}{2}\mathcal{N}(\theta, 0.01\cdot I),$$ where $$\theta \in \mathbb{R}^2$$ and $$n=1$$. We proceed as follows:

1. We construct a training set $$\mathcal{T}_{\text{train}} = \{(\theta_i, X_i)\}_{i=1}^B \sim p(X \mid \theta)\pi(\theta)$$ with $$B=50{,}000$$ and $$\pi(\theta) = \mathcal{N}(0, 2I)$$ to learn $$\hat{\pi}(\theta \mid X)$$ through a generative model. For this example, we use a flow matching posterior estimator, whose idea was first introduced in \citepsupp{lipman_flow_2023} and then adapted for simulation-based inference settings in \citepsupp{wildberger_flow_2023}. We leverage the implementation available in the `SBI` library \citepsupp{tejero-cantero_sbi_2020}, using default hyper-parameters;
2. We construct a calibration set $$\mathcal{T}_{\text{cal}} = \{(\theta_i, X_i)\}_{i=1}^{B^\prime} \sim p(X \mid \theta)r(\theta)$$ with $$B^{\prime}=30{,}000$$ and $$r(\theta) = \mathcal{U}([-10, 10] \times [-10, 10])$$ to learn a monotonic transformation $$\hat{F}(\hat{\pi}(\theta \mid X);\theta)$$ of the estimated posterior. Here, we again estimate an amortized p-value function $$\P_{X \mid \theta}\left( \hat{\pi}(\theta \mid X) < \hat{\pi}(\theta_0 \mid X) \right)$$ according to Algorithm~\ref{algo:rejection_prob0} by setting the number of resampled cutoffs to $$K=10$$ and leveraging a tree-based gradient-boosted probabilistic classifier as implemented in the `CatBoost` library \citepsupp{prokhorenkova_catboost_2018}. We only optimize the number of trees and the maximum depth, which are finally set to $$1000$$ and $$9$$, respectively;
3. We then generate two observations to represent poor alignment with the prior distribution --- $$X_{1, \text{target}} \sim p(X \mid \theta^\star = [8.5, -8.5])$$ and $$X_{2, \text{target}} \sim p(X \mid \theta^\star = [-8.5, -8.5])$$ --- and one observation to represent good alignment with the prior distribution --- $$X_{3, \text{target}} \sim p(X \mid \theta^\star = [0, 0])$$ --- for which we again construct HPD and FreB sets. As in the previous example, we only observe a single sample to infer $$\theta^\star$$, i.e., $$n=1$$;
4. We check local coverage as detailed in  Appendix~\ref{sec:diagnostics} by first generating a diagnostic set $$\mathcal{T}_{\text{diagn}} = \{(\theta_i, X_i)\}_{i=1}^{B^{\prime\prime}} \sim p(X \mid \theta)r(\theta)$$ with $$B^{\prime\prime}=20{,}000$$ and $$r(\theta) = \mathcal{U}([-10, 10] \times [-10, 10])$$ and then learning a probabilistic classifier via a bivariate Generalized Additive Model (GAM) with thin plate splines as implemented in the `MGCV` library in `R` \citepsupp{wood_package_2015}.